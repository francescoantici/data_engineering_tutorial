{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igQAlRDcTx_i"
      },
      "source": [
        "# Data Loading, Clustering, and Machine Learning Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdv9dqSSZj1X"
      },
      "source": [
        "## Class Summary\n",
        "\n",
        "In this class, we will explore a publicly available dataset and apply various techniques to analyze and model the data. The session will be structured as follows:\n",
        "\n",
        "1. **Data Loading and Inspection**: We will begin by loading the dataset and performing an initial inspection to understand its structure, features, and any potential issues that may need to be addressed before analysis.\n",
        "  \n",
        "2. **Clustering**: We will then dive into clustering techniques, focusing on how to effectively perform clustering on selected data features. This step is instrumental for data characterization and analysis, as it allows us to group similar data together and identify patterns in the dataset.\n",
        "\n",
        "3. **Machine Learning Models**: Finally, we will apply various Machine Learning (ML) models to the data, focusing on both regression and classification tasks. We will experiment with different algorithms and evaluate their performance.\n",
        "\n",
        "Throughout the class, we will highlight best practices for data analysis, clustering, and predictive modeling, with a focus on how to handle data effectively for such applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Meg1WZWqb_SC"
      },
      "source": [
        "## Preliminaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC3mRz9qcGDV"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAiONZb4TtIH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8VNNSRkWejK"
      },
      "source": [
        "### PM100 Dataset\n",
        "\n",
        "PM100 is a dataset available at https://zenodo.org/records/10127767. The dataset contains the data of more than **230K job executed on Marconi100 supercomputer**. A job is a **user-submitted request** to execute a specific computational task on an HPC system. These jobs are managed by a scheduler, which allocates the necessary resources—such as CPU cores, memory, and time—based on availability and predefined policies. This ensures efficient and fair utilization of the HPC infrastructure.\n",
        "\n",
        "Each entry contains information job executions (i.e., a computational task executed on a supercomputer), such as **power consumption, duration, user**, etc. ​"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KYz1gFqTVcP7"
      },
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "!wget https://zenodo.org/records/10127767/files/job_table.parquet?download=1 -O job_table.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia90w---X2fi"
      },
      "source": [
        "## Understanding Parquet Files and Loading Them with Pandas\n",
        "\n",
        "### What is a Parquet File?\n",
        "\n",
        "A **Parquet file** is a columnar storage file format optimized for efficiency in big data processing. It is designed to handle large datasets efficiently and is commonly used in data analytics and machine learning workflows.\n",
        "\n",
        "#### Key Features of Parquet:\n",
        "- **Columnar Storage:** Data is stored column-wise rather than row-wise, improving compression and query performance.\n",
        "- **Efficient Compression:** Parquet uses efficient encoding and compression techniques (e.g., Snappy, Gzip, Brotli) to reduce storage size.\n",
        "- **Schema Evolution:** It supports schema evolution, allowing for flexible data modifications.\n",
        "- **Optimized for Analytics:** Queries that select specific columns can run faster because only the required columns are read from disk.\n",
        "\n",
        "### How to Load a Parquet File with Pandas\n",
        "\n",
        "Pandas provides built-in support for reading and writing Parquet files through the `pyarrow` or `fastparquet` libraries.\n",
        "\n",
        "#### 1. Installing Dependencies\n",
        "To work with Parquet files in Pandas, install one of the required backends:\n",
        "\n",
        "```\n",
        "pip install pandas pyarrow  # or pip install pandas fastparquet\n",
        "```\n",
        "\n",
        "#### 2. Pandas API\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "df = pd.read_parquet(PATH_TO_PARQUET_FILE)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrOCD49qXD_0"
      },
      "outputs": [],
      "source": [
        "# Load Dataset in a DataFrame format\n",
        "df = pd.read_parquet('job_table.parquet')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRXxGIp3vScA"
      },
      "outputs": [],
      "source": [
        "# Display info on the data columns\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jEgKSznvU4q"
      },
      "outputs": [],
      "source": [
        "# Generate statistics on the data columns\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8Q9NZbZv886"
      },
      "source": [
        "## Data Pre-processing\n",
        "\n",
        "Here, we will perform some data pre-processing on the original data, which is intrumental for the following steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sA94UVHv_O9"
      },
      "source": [
        "### Job Power Consumption\n",
        "\n",
        "Each job entry contains information on the power consumption of the job execution. The feature is a time-serie of the power consumption, sampled every 20 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9oZXmMnwLlr"
      },
      "outputs": [],
      "source": [
        "df[\"node_power_consumption\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru9Pfewtz2br"
      },
      "source": [
        "We have to check that all the elements are not empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqicSldyyUA9"
      },
      "outputs": [],
      "source": [
        "# Sanity check\n",
        "df_empty = df[df[\"node_power_consumption\"].apply(lambda pc: len(pc) == 0)]\n",
        "print(df_empty)\n",
        "if len(df_empty):\n",
        "  df = df[df[\"node_power_consumption\"].apply(lambda pc: len(pc) != 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjBU0n1zz7GG"
      },
      "source": [
        "#### Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xj3-9Euowc0O"
      },
      "outputs": [],
      "source": [
        "# Plot 5 random jobs' power consumption\n",
        "df_plot = df.sample(n = 5)\n",
        "df_plot[\"x\"] = df_plot[\"node_power_consumption\"].apply(lambda x: list(range(len(x))))\n",
        "for i in range(len(df_plot)):\n",
        "    plt.plot(df_plot[\"x\"].iloc[i], df_plot[\"node_power_consumption\"].iloc[i], label = f\"Job {i}\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Power Consumption\")\n",
        "plt.title(\"Power Consumption of 5 Random Jobs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4hK1eltzuRY"
      },
      "source": [
        "#### Normalization\n",
        "\n",
        "For our purposes we want to inspect only the average power consumption per job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71sPV7WE0ThW"
      },
      "outputs": [],
      "source": [
        "# Creation of an additional feature\n",
        "df[\"average_power_consumption\"] = df[\"node_power_consumption\"].apply(lambda x: np.mean(x))\n",
        "df[\"average_power_consumption\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nczwDfTK0ixJ"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df[\"average_power_consumption\"].values, bins = 50, kde = False)\n",
        "plt.title(\"Average Power Consumption Distribution\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.xlabel(\"Average Power Consumption\")\n",
        "# plt.yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtZBKCPZ1eWt"
      },
      "source": [
        "Since the values span a **very large range,** we can **normalize the average power consumption** to bring the data within a manageable scale. Additionally, since jobs run on a varying number of nodes, we can better characterize each job by its **performance on a single node**. Therefore, we **normalize the power consumption based on the number of nodes allocated to each job**, providing a more consistent and comparable measure of energy usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQH4y7oP1vys"
      },
      "outputs": [],
      "source": [
        "# Normalization\n",
        "df[\"norm_average_power_consumption\"] = df[\"average_power_consumption\"] / df[\"num_nodes_alloc\"]\n",
        "df[\"norm_average_power_consumption\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcf6ZF8o1-mW"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df[\"norm_average_power_consumption\"].values, bins = 50, kde = False)\n",
        "plt.title(\"Average Power Consumption (Per Node) Distribution\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.xlabel(\"Average Power Consumption Per Node\")\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8CUwBbb2awG"
      },
      "source": [
        "## Data Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gevujQ1A2gzX"
      },
      "source": [
        "### Job Characterization\n",
        "\n",
        "In order to perform analysis and characterize the jobs based on their per-node power consumption, we can divide them into categories through clustering techniques. For our experiments we will be using the **K-Means** and the **DBSCAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTtwgF2T7Fcz"
      },
      "source": [
        "#### DBSCAN\n",
        "\n",
        "**DBSCAN** is a popular **density-based** clustering algorithm used in machine learning. Unlike algorithms like **K-Means**, DBSCAN does not require the number of clusters to be specified in advance. Instead, it forms clusters based on the density of points in a region, marking regions of low density as **outliers** or **noise**.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Core Points**: Points that have a sufficient number of neighboring points within a given radius (`eps`).\n",
        "- **Border Points**: Points that are within the `eps` radius of a core point but do not have enough neighbors themselves to be considered core points.\n",
        "- **Noise Points**: Points that do not belong to any cluster because they do not meet the criteria of being close enough to core points.\n",
        "\n",
        "DBSCAN is effective for finding clusters of arbitrary shapes, making it a versatile choice for various datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRndt1so72NF"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUDJ2YqV579J"
      },
      "outputs": [],
      "source": [
        "dbscan = DBSCAN()\n",
        "df[\"cluster_db\"] = dbscan.fit_predict(df[\"norm_average_power_consumption\"].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c13liFY6WS-"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df, x = \"norm_average_power_consumption\", bins = 50, kde = False, hue = \"cluster_db\")\n",
        "plt.title(\"Average Power Consumption (Per Node) Distribution\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.xlabel(\"Average Power Consumption Per Node\")\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWfF7g7b_T04"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df, x = \"cluster_db\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.title(\"DBSCAN Clustering\")\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1r4I8gj41QO"
      },
      "source": [
        "#### K-Means\n",
        "\n",
        "### K-Means Clustering\n",
        "\n",
        "**K-Means** is one of the most widely used **partitioning-based** clustering algorithms. It aims to divide a dataset into a predefined number of clusters (K) by assigning each data point to the nearest cluster center (centroid). The algorithm iteratively refines the centroids to minimize the **within-cluster sum of squared distances** (inertia).\n",
        "\n",
        "### Key Concepts:\n",
        "- **Centroids**: The center points of each cluster, computed as the mean of the data points assigned to that cluster.\n",
        "- **Clusters**: Groups of data points that are closer to the same centroid than to any other centroid.\n",
        "- **Iterations**: The algorithm repeatedly updates the centroids by moving them to the average position of the points in the cluster until convergence (no significant changes in centroid positions).\n",
        "\n",
        "K-Means is effective when clusters are roughly spherical and evenly sized, but it requires the number of clusters (K) to be specified beforehand, which can be a limitation.\n",
        "\n",
        "### Steps:\n",
        "1. Initialize K centroids randomly.\n",
        "2. Assign each data point to the nearest centroid.\n",
        "3. Recalculate the centroids based on the mean of the points in each cluster.\n",
        "4. Repeat the assignment and centroid update steps until the centroids no longer change significantly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cI6fkSZY7ZKC"
      },
      "source": [
        "##### Elbow Method\n",
        "\n",
        "In order to find the optimal number of clusters (i.e., $k$) for the K-Means, we can rely on the Elbow Method. Such method analyzes the inertia (sum of squared distances from points to cluster centers) to determine which is the $k$ after which the inertia starts decreasing at a slower rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ods-eqcu77To"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "!pip3 install kneed\n",
        "from kneed import KneeLocator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Sq5z6Zv3R4m"
      },
      "outputs": [],
      "source": [
        "def elbow_method(data, clustering_method, k_min, k_max):\n",
        "  # Store the sum of squared distances (inertia)\n",
        "  inertia = []\n",
        "\n",
        "  k_values = list(range(k_min, k_max + 1))\n",
        "\n",
        "  # Compute K-Means clustering for each value of K\n",
        "  for k in k_values:\n",
        "    kmeans = clustering_method(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(data)\n",
        "    inertia.append(kmeans.inertia_)  # Store inertia (sum of squared distances)\n",
        "\n",
        "  kneedle = KneeLocator(k_values, inertia, curve=\"convex\", direction=\"decreasing\")\n",
        "\n",
        "  # Plot the Elbow curve\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(k_values, inertia, marker=\"o\", linestyle=\"--\")\n",
        "\n",
        "  # Draw a vertical line at optimal k\n",
        "  optimal_k = kneedle.knee\n",
        "  plt.axvline(x=optimal_k, color='red', linestyle='--', label=\"Optimal k\")\n",
        "\n",
        "  plt.legend()\n",
        "  plt.xlabel(\"Number of Clusters (K)\")\n",
        "  plt.ylabel(\"Inertia (Sum of Squared Distances)\")\n",
        "  plt.title(\"Elbow Method for Optimal K\")\n",
        "  plt.xticks(k_values)\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()\n",
        "  # Return optimal k\n",
        "  return optimal_k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6jYHVtl2_IS"
      },
      "outputs": [],
      "source": [
        "opt_k = elbow_method(data = df[\"norm_average_power_consumption\"].values.reshape(-1, 1), clustering_method = KMeans, k_min = 1, k_max = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8__Kjh82_hz"
      },
      "outputs": [],
      "source": [
        "# Generate the clusters\n",
        "kmeans = KMeans(n_clusters=opt_k, random_state=42)\n",
        "df[\"cluster\"] = kmeans.fit_predict(df[\"norm_average_power_consumption\"].values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHXBAzzV6Lqw"
      },
      "source": [
        "Let's now see how the data are split by the K-Means."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq-uXaOK571o"
      },
      "outputs": [],
      "source": [
        "sns.histplot(data = df, x = \"norm_average_power_consumption\", bins = 50, kde = False, hue = \"cluster\")\n",
        "plt.title(\"Average Power Consumption (Per Node) Distribution\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.xlabel(\"Average Power Consumption Per Node\")\n",
        "plt.yscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsrel1u-_vIF"
      },
      "outputs": [],
      "source": [
        "df[\"cluster\"] = df.cluster.astype(str)\n",
        "sns.histplot(data = df, x = \"cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"# of Jobs\")\n",
        "plt.title(\"K-Means Clustering\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nsHETr28A7P"
      },
      "source": [
        "#### Considerations\n",
        "\n",
        "With **DBSCAN** we obtain too many labels, resulting in not having clear and meaningful labels. Conversely, with **K-Means**, the resulting labels make more sense. Indeed, the cluster labels effectively categorize the jobs into three groups: \"low,\" \"medium,\" and \"high\" power-consuming jobs, providing a clearer and more intuitive division of the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwYNCnHa9I8T"
      },
      "source": [
        "## ML Predictive Modelling\n",
        "\n",
        "In this section we will see how to use several **ML models** to perform prediction on the data.\n",
        "We will showcase how to perform **regression and classification tasks**. For the regression, we will predict the power consumption values in **\"norm_average_power_consumption\"**; while for the classification we will predict the **\"cluster\"** label created through the **K-Means**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Li_dvefU-fbH"
      },
      "source": [
        "### Data Split and Preparation for Modelling\n",
        "\n",
        "In order to perform prediction tasks, we need to prepare the data for the models. This includes defining the target values, the input features and splitting the data into training and test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4eRCGQb-5YS"
      },
      "source": [
        "#### Target Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYDw_iN9-9V6"
      },
      "outputs": [],
      "source": [
        "y_regression = df[\"norm_average_power_consumption\"].values\n",
        "y_classification = df[\"cluster\"].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVCArsCFAGew"
      },
      "source": [
        "#### Input Features\n",
        "\n",
        "### Selecting Input Features for a Tabular Prediction Task\n",
        "\n",
        "Selecting the right input features is crucial for building an effective machine learning model for a tabular prediction task. The goal is to choose features that are most informative and relevant to the prediction target, while minimizing noise and redundancy.\n",
        "\n",
        "### Steps for Feature Selection:\n",
        "\n",
        "1. **Domain Knowledge**:\n",
        "   - Leverage knowledge on the domain to select features that are likely to be important for the target variable.\n",
        "\n",
        "2. **Correlation Analysis**:\n",
        "   - Use correlation matrices to identify and remove highly correlated features. Features that are highly correlated with each other can introduce multicollinearity, leading to less stable models.\n",
        "\n",
        "3. **Feature Importance**:\n",
        "   - Use models like **Random Forest** or **Gradient Boosting** to calculate feature importance scores. These models can help identify which features contribute the most to the prediction.\n",
        "\n",
        "4. **Statistical Methods**:\n",
        "   - Perform statistical tests (e.g., chi-squared test for categorical features, ANOVA for continuous features) to assess the relationship between each feature and the target variable.\n",
        "\n",
        "5. **Dimensionality Reduction**:\n",
        "   - Techniques like **Principal Component Analysis (PCA)** or **t-SNE** can help reduce the feature space, especially when dealing with high-dimensional datasets, while retaining the most important variance.\n",
        "\n",
        "6. **Feature Engineering**:\n",
        "   - Consider creating new features from existing ones. For example, combining categorical features or extracting new insights from continuous variables can improve model performance.\n",
        "\n",
        "7. **Remove Redundant Features**:\n",
        "   - Drop irrelevant or redundant features that do not add predictive value to the model or may increase overfitting.\n",
        "\n",
        "### Considerations:\n",
        "- Always keep track of how changes to feature selection impact model performance through cross-validation or separate validation datasets.\n",
        "- Be mindful of the risk of **overfitting** when using too many features, especially when dealing with limited data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iac1WjFBASF0"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1846BVkAmza"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "eligible_features = [\"group_id\", \"job_state\", \"num_nodes_alloc\", \"num_cores_alloc\", \"num_tasks\", \"partition\", \"qos\", \"run_time\", \"shared\", \"threads_per_core\", \"time_limit\", \"num_gpus_alloc\", \"mem_alloc\", \"user_id\"]\n",
        "\n",
        "for feat in eligible_features:\n",
        "  df[feat] = df[feat].astype(\"category\").cat.codes\n",
        "\n",
        "targets = [\"cluster\", \"norm_average_power_consumption\"]\n",
        "\n",
        "corr_matrix = df[eligible_features + targets].corr()\n",
        "\n",
        "# Display the correlation matrix\n",
        "print(corr_matrix[targets])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9RGEw0AAp3Z"
      },
      "outputs": [],
      "source": [
        "# Plot the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr_matrix[targets], annot=True, cmap='coolwarm', fmt='.2f', cbar=True)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtDiRoc2AFCQ"
      },
      "outputs": [],
      "source": [
        "# We pick the features which are more correlated to the targets\n",
        "feature_set = [\"job_state\", \"num_cores_alloc\", \"partition\", \"qos\", \"run_time\", \"shared\", \"time_limit\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0Wbc3CGEpyJ"
      },
      "source": [
        "#### Data Normalization\n",
        "\n",
        "Data normalization is a crucial preprocessing step in machine learning, especially when using algorithms that are sensitive to the scale of the input features, such as **K-Nearest Neighbors**, **Support Vector Machines**, and **Neural Networks**. Normalization transforms the features of the dataset into a standard scale, ensuring that no feature dominates over others due to differences in magnitude or units. Common methods include **Min-Max Scaling**, which scales the data to a range between 0 and 1, and **Standardization**, which rescales the data to have a mean of 0 and a standard deviation of 1. Properly normalized data can significantly improve the performance and convergence of machine learning models, helping them to make more accurate predictions.\n",
        "\n",
        "For this example, we will be using the **Min-Max** scailing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8lg3gUrErsa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Create the vector of the input features\n",
        "x = scaler.fit_transform(df[feature_set].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ2sIAAWEvqQ"
      },
      "outputs": [],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmUf3AYCDPRI"
      },
      "source": [
        "#### Data Splitting\n",
        "\n",
        "We split the data into **training and test set**, with a **70/30** proportion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoWVsj_ODUQ7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.3\n",
        "\n",
        "# Subsampling if the computation is too heavy on the resources\n",
        "subsampling_ratio = 1 # set to 1 if not needed\n",
        "subsampling_elem = int(len(df) * subsampling_ratio)\n",
        "\n",
        "# Split the data into training and testing sets (70% training, 30% testing)\n",
        "x_train_reg, x_test_reg, y_train_reg, y_test_reg = train_test_split(x[:subsampling_elem], y_regression[:subsampling_elem], test_size=test_size, random_state=42)\n",
        "x_train_clas, x_test_clas, y_train_clas, y_test_clas = train_test_split(x[:subsampling_elem], y_classification[:subsampling_elem], test_size=test_size, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XFpLg2YUiI7"
      },
      "source": [
        "### Modeling\n",
        "\n",
        "We demonstrate how to perform regression and classification tasks with different ML models, such as the:\n",
        "\n",
        "- **Linear Regression (LR)**: Linear regression is a simple statistical model that establishes a relationship between input features and a continuous target variable. It assumes that the relationship between the features and the target is linear, and it aims to minimize the sum of squared errors between the predicted and actual values.\n",
        "  \n",
        "- **Decision Tree (DT)**: A decision tree is a hierarchical model that splits the dataset into subsets based on feature values, forming a tree-like structure. Each internal node represents a feature, and each leaf node represents a predicted output. It is easy to interpret but can suffer from overfitting.\n",
        "\n",
        "- **Random Forest (RF)**: A random forest is an ensemble method that creates multiple decision trees by bootstrapping data samples and using random subsets of features for each tree. The final prediction is made by averaging the outputs of all trees, reducing overfitting and improving accuracy compared to a single decision tree.\n",
        "\n",
        "- **Support Vector Machine (SVM)**: SVM is a supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates data points of different classes. For regression, it tries to fit the best possible line while maximizing the margin between the line and the data points.\n",
        "\n",
        "- **K-Nearest Neighbors (KNN)**: KNN is a simple, non-parametric algorithm that classifies data points based on the majority label of their nearest neighbors. It computes the distance between data points and assigns a label to a point based on the labels of its K closest neighbors, making it highly intuitive but computationally expensive for large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AddVfQgkUtPo"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.svm import SVR, SVC\n",
        "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report, f1_score, accuracy_score, mean_absolute_percentage_error, ConfusionMatrixDisplay, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BORcpFyKVWan"
      },
      "source": [
        "#### Regression Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg99R-wpRDyy"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "models_reg = {\n",
        "    'Linear Regression': LinearRegression(n_jobs = -1),\n",
        "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
        "    'Random Forest Regressor': RandomForestRegressor(n_jobs = -1),\n",
        "    # 'Support Vector Regressor': SVR(),\n",
        "    'K-Neighbors Regressor': KNeighborsRegressor(n_jobs = -1)\n",
        "}\n",
        "\n",
        "# Train, predict, and evaluate each model\n",
        "results = []\n",
        "\n",
        "for name, model in models_reg.items():\n",
        "  # Fit the model\n",
        "  model.fit(x_train_reg, y_train_reg)\n",
        "\n",
        "  # Make predictions\n",
        "  y_pred_reg = model.predict(x_test_reg)\n",
        "\n",
        "  # Calculate evaluation metrics\n",
        "  mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
        "  mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "  mape = mean_absolute_percentage_error(y_test_reg, y_pred_reg)\n",
        "  r2 = r2_score(y_test_reg, y_pred_reg)\n",
        "\n",
        "  # Store the results\n",
        "  results.append({\n",
        "      'Model': name,\n",
        "      'MAE': mae,\n",
        "      'MAPE': mape,\n",
        "      'MSE': mse,\n",
        "      'R2': r2\n",
        "  })\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "results_df_reg = pd.DataFrame.from_records(results)\n",
        "print(results_df_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBCOk8bjSUU-"
      },
      "source": [
        "#### Visualization of the results\n",
        "\n",
        "We plot the results to help evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4JSTr45SfV9"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 4, figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "metrics = [\"MAE\", \"MAPE\", \"MSE\", \"R2\"]\n",
        "\n",
        "for i in range(len(metrics)):\n",
        "  ax = axes[i]\n",
        "  metric = metrics[i]\n",
        "  sns.barplot(data = results_df_reg, x = \"Model\", y = metric, ax = ax)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diC-iLjGVZF2"
      },
      "source": [
        "#### Classification Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEYglEqyVawr"
      },
      "outputs": [],
      "source": [
        "# Initialize the models\n",
        "models_clas = {\n",
        "    'Decision Tree Regressor': DecisionTreeClassifier(),\n",
        "    'Random Forest Regressor': RandomForestClassifier(n_jobs = -1),\n",
        "    # 'Support Vector Regressor': SVC(),\n",
        "    'K-Neighbors Regressor': KNeighborsClassifier(n_jobs = -1)\n",
        "}\n",
        "\n",
        "# Train, predict, and evaluate each model\n",
        "results = []\n",
        "\n",
        "for name, model in models_clas.items():\n",
        "  # Fit the model\n",
        "  model.fit(x_train_clas, y_train_clas)\n",
        "\n",
        "  # Make predictions\n",
        "  y_pred_clas = model.predict(x_test_clas)\n",
        "\n",
        "  # Calculate evaluation metrics\n",
        "  f1 = f1_score(y_test_clas, y_pred_clas, average = \"macro\")\n",
        "  accuracy = accuracy_score(y_test_clas, y_pred_clas)\n",
        "  cr = classification_report(y_test_clas, y_pred_clas)\n",
        "\n",
        "  print(f\"Classification Report of {name}:\\n{cr}\")\n",
        "\n",
        "  # Store the results\n",
        "  results.append({\n",
        "      'Model': name,\n",
        "      'F1': f1,\n",
        "      'Accuracy': accuracy\n",
        "  })\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "results_df_clas = pd.DataFrame.from_records(results)\n",
        "print(results_df_clas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2m-CihUWSQk"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
        "axes = axes.flatten()\n",
        "metrics = [\"F1\", \"Accuracy\"]\n",
        "\n",
        "for i in range(len(metrics)):\n",
        "  ax = axes[i]\n",
        "  metric = metrics[i]\n",
        "  sns.barplot(data = results_df_clas, x = \"Model\", y = metric, ax = ax)\n",
        "  ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Error Analysis for Regression and Classification Tasks\n",
        "\n",
        "When evaluating the performance of the models, it is also important to understand how the metrics are reflected in the actual prediction error. \n",
        "For the regression task, we will investigate the residual analysis, error distribution, and performance metrics.\n",
        "While for the classification, we will take a look at the confusion matrices, class-wise metrics, and error inspection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Residual plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Absolute residual\n",
        "fig, axes = plt.subplots(1, len(models_reg), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_reg.items():\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_reg)\n",
        "  # Calculate residuals\n",
        "  residuals = y_test_reg - y_pred_model\n",
        "  ax = axes[m_idx]\n",
        "  sns.scatterplot(x=y_pred_model, y=residuals, ax = ax)\n",
        "  ax.axhline(0, color='red', linestyle='--')\n",
        "  ax.set_title(f\"Residual Plot for {name}\")\n",
        "  ax.set_xlabel(\"Predicted values\")\n",
        "  ax.set_ylabel(\"Residuals\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When displaying absolute errors with regression, it is not always easy to the reader to evaluate such error. A way to ease the understanding of the performance is to put all the errors in percentual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Percentage residual\n",
        "fig, axes = plt.subplots(1, len(models_reg), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_reg.items():\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_reg)\n",
        "  # Calculate residuals\n",
        "  residuals = (np.abs(y_test_reg - y_pred_model)/y_test_reg) * 100\n",
        "  ax = axes[m_idx]\n",
        "  sns.scatterplot(x=y_pred_model, y=residuals, ax = ax)\n",
        "  ax.set_title(f\"Residual Plot for {name}\")\n",
        "  ax.set_xlabel(\"Predicted values\")\n",
        "  ax.set_ylabel(\"Residuals (in %)\")\n",
        "  ax.set_ylim(0, 100)\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Error distribution\n",
        "\n",
        "The scatterplot is a good way of visualizing the error, however it is non-trivial to understand its distribution. Hence, a good way to evaluate this is to plot an histogram of the residuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "fig, axes = plt.subplots(1, len(models_reg), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_reg.items():\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_reg)\n",
        "  # Calculate residuals\n",
        "  residuals = y_test_reg - y_pred_model\n",
        "  ax = axes[m_idx]\n",
        "  sns.histplot(residuals, bins=30, ax = ax)\n",
        "  ax.set_title(\"Distribution of Residuals\")\n",
        "  ax.set_xlabel(\"Residual\")\n",
        "  ax.set_ylabel(\"Frequency\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Same as before, we also visualize the error in %"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(models_reg), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_reg.items():\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_reg)\n",
        "  # Calculate residuals\n",
        "  residuals = (np.abs(y_test_reg - y_pred_model)/y_test_reg) * 100\n",
        "  ax = axes[m_idx]\n",
        "  sns.histplot(residuals, bins=50, ax = ax)\n",
        "  ax.set_title(\"Distribution of Residuals\")\n",
        "  ax.set_xlabel(\"Residual\")\n",
        "  ax.set_ylabel(\"Frequency\")\n",
        "  ax.set_yscale(\"log\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the classification task, we can visualize the confusion matrix and the error per class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(models_clas), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_clas.items():\n",
        "  ax = axes[m_idx]\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_clas)\n",
        "  # Calculate confusion matrix\n",
        "  cm = confusion_matrix(y_test_clas, y_pred_model)\n",
        "  disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "  disp.plot(cmap='Blues', ax = ax)\n",
        "  ax.set_title(f\"Confusion Matrix {name}\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We show another way of analysing performance per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Per-Class Error Analysis ---\n",
        "fig, axes = plt.subplots(1, len(models_clas), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_clas.items():\n",
        "  ax = axes[m_idx]\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_clas)\n",
        "  errors = pd.DataFrame({\n",
        "    \"True Label\": y_test_clas,\n",
        "    \"Predicted Label\": y_pred_model\n",
        "  })\n",
        "  errors[\"Correct\"] = errors[\"True Label\"] == errors[\"Predicted Label\"]\n",
        "  # Calculate confusion matrix\n",
        "  sns.countplot(data=errors, x=\"True Label\", hue=\"Correct\", ax = ax)\n",
        "  ax.set_title(f\"Correct vs Incorrect Predictions per Class, with {name}\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "if setting the stats parameters of the barplot to \"percent\", we can report the same plot with percentual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(models_clas), figsize=(40, 8))\n",
        "axes = axes.flatten()\n",
        "m_idx = 0\n",
        "for name, model in models_clas.items():\n",
        "  ax = axes[m_idx]\n",
        "  # Make predictions\n",
        "  y_pred_model = model.predict(x_test_clas)\n",
        "  errors = pd.DataFrame({\n",
        "    \"True Label\": y_test_clas,\n",
        "    \"Predicted Label\": y_pred_model\n",
        "  })\n",
        "  errors[\"Correct\"] = errors[\"True Label\"] == errors[\"Predicted Label\"]\n",
        "  # Calculate confusion matrix\n",
        "  sns.countplot(data=errors, x=\"True Label\", hue=\"Correct\", ax = ax, stat=\"percent\")\n",
        "  ax.set_title(f\"Correct vs Incorrect Predictions per Class, with {name}\")\n",
        "  m_idx+=1\n",
        "\n",
        "plt.show()\n",
        "plt.clf()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pyenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
